{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paprika Database\n",
    "\n",
    "This script connects to Paprika app's SQLite database and pull out whatever we want and format it as JSON, YAML, whatever.\n",
    "\n",
    "## NOTE:\n",
    "Before running the first time:\n",
    "1. create a file in the same directory as this Notebook, named \"config.py\"\n",
    "2. copy paste this line:\n",
    "\n",
    "path_project    = \"/local/path/to/this/repo/joi.github.io\"\n",
    "\n",
    "_(which should be the path to the direcotry one level up from where this file here is.)_\n",
    "\n",
    "3. pip install commonmark | NEW!\n",
    "4. pip install Unidecode | https://pypi.org/project/Unidecode/\n",
    "5. pip install pathvalidate | https://pypi.org/project/pathvalidate/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS -------------------------------------\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "import json\n",
    "import commonmark\n",
    "import pprint\n",
    "from pathvalidate import sanitize_filename\n",
    "import unidecode\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "from pathlib import Path\n",
    "from shutil import copyfile\n",
    "import config # This imports our local config file, \"config.py\". Access vars like so: config.var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARS -----------------------------------------\n",
    "\n",
    "# Date time stamp\n",
    "now = datetime.now()\n",
    "dt = now.strftime(\"%Y-%m-%d %H.%M\")\n",
    "#print(dt)\n",
    "\n",
    "# Environment-dependent Paths\n",
    "path_project    = config.path_project # manually set in config.py, which is NOT checked into GIT.\n",
    "path_user_home  = str(Path.home())    # automatically detects the User home directory path from the OS/ENV\n",
    "\n",
    "# Paprika Database Path\n",
    "filename_db     = 'Paprika.sqlite'\n",
    "path_paprika    = '/Library/Group Containers/72KVKW69K8.com.hindsightlabs.paprika.mac.v3/Data/'\n",
    "path_db_sub     = path_paprika + 'Database/'\n",
    "path_db_med     = path_user_home + path_db_sub\n",
    "path_db_full    = path_user_home + path_db_sub + filename_db\n",
    "path_photos     = path_user_home + path_paprika + 'Photos/'\n",
    "\n",
    "# Paprika Databse Backup. / We create this before we do anything, just in case.\n",
    "file_db_bu          = 'Paprika-BU-' + dt + '.sqlite'\n",
    "path_db_bu_sub      = path_project + '/__scripts/backups/'\n",
    "path_db_bu          = path_db_bu_sub + file_db_bu + '.zip'\n",
    "\n",
    "# Replaces Above \"Working\"\n",
    "path_temp_working   = path_db_bu_sub + \"tmp/\" # put tmp in backup\n",
    "path_db_working     = path_temp_working + filename_db\n",
    "\n",
    "# Output\n",
    "path_output_json_data = path_project + '/_data/'\n",
    "path_output_json_files = path_project + '/_data/recipes/'\n",
    "path_output_mkdn_files = path_project + '/_recipes/'\n",
    "path_output_phot_files = path_project + '/images/recipes/'\n",
    "\n",
    "# Paparika Timestamp Offset: 978307200\n",
    "ts_offset = 978307200\n",
    "\n",
    "\n",
    "# - FUNCTIONS ----------------------------------\n",
    "\n",
    "# Utility Functions ----------------------------\n",
    "\n",
    "# Clobber a string into a filename -------------\n",
    "def make_filename(string):\n",
    "    string = unidecode.unidecode(string)\n",
    "    # Need to strip out amperstands. See content.html liquid too.\n",
    "    string = string.replace(\" &\",\"\")\n",
    "    string = string.replace(\" \",\"-\")\n",
    "    #string = created[0:10] + \"-\" + string\n",
    "    #string=str(bytes(string, 'utf-8').decode('utf-8','ignore').encode(\"utf-8\",'ignore'))\n",
    "    #string=string.replace(\"b'\",\"\").replace(\"'\",\"\")\n",
    "    invalid = '<>:\"/\\|?* ,()‚Äú‚Äù‚Äò‚Äô\\''\n",
    "    for char in invalid:\n",
    "        string = string.replace(char, '')\n",
    "    string = sanitize_filename(string)\n",
    "    \n",
    "    return string\n",
    "\n",
    "# Delete and Create Output Directories\n",
    "def output_directories(path):\n",
    "  if os.path.exists(path):\n",
    "    shutil.rmtree(path, ignore_errors=True)\n",
    "  os.mkdir(path)\n",
    "\n",
    " # Turn a multiline text block into a Markdown List\n",
    "def make_list(text):\n",
    "    return \"* \" + text.replace(\"\\n\", \"\\n* \") \n",
    "\n",
    "# Database Functions ---------------------------\n",
    "\n",
    "# Connect to Database\n",
    "def db_connect(db_file):\n",
    "    #conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        # row_factory does some magic for us\n",
    "        # see: https://stackoverflow.com/questions/3300464/how-can-i-get-dict-from-sqlite-query\n",
    "        conn.row_factory = sqlite3.Row\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    return conn\n",
    "\n",
    "\n",
    "# Parse Paprika Markdown-ish into Markdown\n",
    "    # if 0 == recipe -> pass 1 to make_filename()\n",
    "    # if 0 == photo -> take 1 as key into {photos_dict} and get filename\n",
    "def paprika_markdownish(content,photos_dict,uid):\n",
    "    if content:\n",
    "        content = re.sub(r'\\[(photo):(.+?)\\]',lambda x: '![' + x.group(2) + '](/images/recipes/' + uid + '/' + photos_dict[x.group(2)] + ')',content)\n",
    "        content = re.sub(r'\\[(recipe):(.+?)\\]',lambda x: '[' + x.group(2) + '](/recipes/' + make_filename(x.group(2)) + ')',content)\n",
    "        return content\n",
    "    else:\n",
    "        raise ValueError(\"content null\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT DESTINATION DIRECTORIES ---------------------\n",
    "\n",
    "# If Output Paths don't exist, create them\n",
    "if not os.path.exists(path_db_bu_sub):\n",
    "    os.mkdir(path_db_bu_sub)\n",
    "\n",
    "#if not os.path.exists(path_output_json_files):\n",
    "#    os.mkdir(path_output_json_files)\n",
    "#if not os.path.exists(path_output_mkdn_files):\n",
    "#    os.mkdir(path_output_mkdn_files)\n",
    "\n",
    "output_directories(path_output_json_files)\n",
    "output_directories(path_output_mkdn_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATABASE BACKUPS -----------------------------------\n",
    "\n",
    "# Make a zipped backup of the DB\n",
    "zipfile.ZipFile(path_db_bu, mode='w').write(path_db_full, arcname=file_db_bu, compress_type=zipfile.ZIP_DEFLATED, compresslevel=9)\n",
    "\n",
    "\n",
    "# Make a temp copy of the DB to work with. We delete it later.\n",
    "#\n",
    "# First, check if the temp folder already exists and if so delete it\n",
    "if os.path.exists(path_temp_working):\n",
    "    shutil.rmtree(path_temp_working, ignore_errors=True) #nuke the temp working dir.\n",
    "\n",
    "copy_DB_Return = shutil.copytree(path_db_med, path_temp_working) # create a var here just to capture the useless out put of the copyfile() function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATABASE OPERATIONS ------------------------------\n",
    "\n",
    "# First Database Operation: WAL Checkpoint\n",
    "conn = db_connect(path_db_working)\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"PRAGMA wal_checkpoint;\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Second Database Operation: Get our recipe Data\n",
    "conn = db_connect(path_db_working)\n",
    "with conn:\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"\"\"\n",
    "SELECT \n",
    "    GROUP_CONCAT(C.ZNAME,\"|\") as `categories`,\n",
    "    R.ZCOOKTIME        as `cook_time`,\n",
    "    R.ZINTRASH        as `intrash`,\n",
    "    datetime(R.ZCREATED + {ts_offset},'unixepoch') as `created`,\n",
    "    R.ZCREATED + {ts_offset}                       as `created_ts`,\n",
    "    R.ZDESCRIPTIONTEXT as `description`,\n",
    "    R.ZDIFFICULTY      as `difficulty`,\n",
    "    R.ZDIRECTIONS      as `directions`,\n",
    "    R.ZINGREDIENTS     as `ingredients`,\n",
    "    R.ZIMAGEURL        as `image_url`,\n",
    "    R.ZNAME            as `name`,\n",
    "    R.ZNOTES           as `notes`,\n",
    "    R.ZNUTRITIONALINFO as `nutritional_info`,\n",
    "    R.ZPHOTO           as `photo`,\n",
    "    R.ZPHOTOLARGE      as `photo_large`,\n",
    "    R.ZPREPTIME        as `prep_time`,\n",
    "    R.ZRATING          as `rating`,\n",
    "    R.ZSERVINGS        as `servings`,\n",
    "    R.ZSOURCE          as `source`,\n",
    "    R.ZSOURCEURL       as `source_url`,\n",
    "    R.ZTOTALTIME       as `total_time`,\n",
    "    R.ZUID             as `uid`,\n",
    "    -- We need to do these SELECTS because\n",
    "    -- otherwise the Category concat\n",
    "    -- replicates itself the number of times\n",
    "    -- there are images. No idea why.\n",
    "    (\n",
    "        SELECT\n",
    "            GROUP_CONCAT(RP.ZFILENAME,\"|\") as filename\n",
    "        FROM\n",
    "            ZRECIPEPHOTO as RP\n",
    "        WHERE\n",
    "            RP.ZRECIPE = R.Z_PK\n",
    "    ) as photos_filenames,\n",
    "    (\n",
    "        SELECT\n",
    "            GROUP_CONCAT(RP.ZNAME,\"|\") as name\n",
    "        FROM\n",
    "            ZRECIPEPHOTO as RP\n",
    "        WHERE\n",
    "            RP.ZRECIPE = R.Z_PK\n",
    "    ) as photos_names\n",
    "\n",
    "FROM\n",
    "    ZRECIPE as R\n",
    "\n",
    "LEFT JOIN    Z_12CATEGORIES AS RC\n",
    "    ON    RC.Z_12RECIPES = R.Z_PK\n",
    "LEFT JOIN    ZRECIPECATEGORY AS C\n",
    "    ON    RC.Z_13CATEGORIES = C.Z_PK\n",
    "\n",
    "WHERE R.ZINTRASH IS 0\n",
    "GROUP BY    R.Z_PK;\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    \n",
    "# --------------------------------------------------------------------------------------\n",
    "# For the next bit with columns and results and dict and zip, see:\n",
    "#    https://stackoverflow.com/questions/16519385/output-pyodbc-cursor-results-as-python-dictionary/16523148#16523148\n",
    "#\n",
    "\n",
    "# This grabs the key (cur.descriptiomn) for us\n",
    "columns = [column[0] for column in cur.description]\n",
    "rows = cur.fetchall()\n",
    "\n",
    "results = []\n",
    "for row in rows:\n",
    "    # and here we glue the key to the value\n",
    "    results.append(dict(zip(columns, row)))\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Create a dict to hold the cats -> recipes dictionary\n",
    "cats = {}\n",
    "for result in results:\n",
    "    result['photos_dict'] = {}\n",
    "    result['photos'] = []\n",
    "    result['html'] = {}\n",
    "    result['type'] = None\n",
    "\n",
    "    # FILENAME : This is our Key between the YAML in Markdown stubs and the JSON Data files\n",
    "    fileName = make_filename(result['name'])\n",
    "\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    # Start of RESULT Items FOR loop {\n",
    "        # ---------------------------------------------------\n",
    "        # Photos Stuff\n",
    "        # We do this first so we have access to the photo data when parsing Paprika Markdown-ish [photo:name]\n",
    "        # Split concatened photo filenames and names into a lists\n",
    "    try:\n",
    "        result['photos_filenames'] = result['photos_filenames'].split('|')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        result['photos_names'] = result['photos_names'].split('|')\n",
    "        # if we have photo_names, zip filenames & names into into a key=value dict\n",
    "        # We will use this for the PMD parse below\n",
    "\n",
    "        result['photos_dict'] = dict(zip(result['photos_names'], result['photos_filenames']))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "        #if result[\"photos\"] == []:\n",
    "        #    result[\"photos\"] = False \n",
    "\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # Directions, Descriptions, Ingredients, Nutritional Info\n",
    "    if result['directions']:\n",
    "      rdirections  = paprika_markdownish(result['directions'],result['photos_dict'],result['uid'])\n",
    "      rdirections  = commonmark.commonmark(rdirections)\n",
    "    else:\n",
    "      rdirections = None\n",
    "\n",
    "    if result['description']:\n",
    "      rdescription = paprika_markdownish(result['description'],result['photos_dict'],result['uid'])\n",
    "      rdescription = commonmark.commonmark(rdescription)\n",
    "    else:\n",
    "      rdescription = None\n",
    "\n",
    "    if result['ingredients']:\n",
    "      list_ing_lines   = paprika_markdownish(result['ingredients'],result['photos_dict'],result['uid'])\n",
    "      list_ing_lines   = re.sub('\\\\\\\\x{0D}','\\n',list_ing_lines)\n",
    "      list_ing_lines   = re.sub('\\n\\n','\\n',list_ing_lines)\n",
    "      list_ing_lines   = make_list(list_ing_lines)\n",
    "      ringredients = commonmark.commonmark(list_ing_lines)\n",
    "    else:\n",
    "      ringredients = None\n",
    "\n",
    "    if result['nutritional_info']:\n",
    "      rnutrition = commonmark.commonmark(str(result['nutritional_info']))\n",
    "    else:\n",
    "      rnutrition = None\n",
    "    if result['notes']:\n",
    "      result['notes'] = paprika_markdownish(result['notes'],result['photos_dict'],result['uid'])\n",
    "      rnotes = commonmark.commonmark(result['notes'])\n",
    "    else:\n",
    "      rnotes = None\n",
    "\n",
    "    result['html'] = {\n",
    "      'directions'  : rdirections,\n",
    "      'description' : rdescription,\n",
    "      'ingredients' : ringredients,\n",
    "      'nutrition'   : rnutrition,\n",
    "      # JI: I added this too but doesn't really do anything yet\n",
    "      'notes'       : rnotes\n",
    "      }\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Categories\n",
    "    # Split concatened categories into a list\n",
    "    if result['categories']:\n",
    "        try:\n",
    "            result['categories'] = result['categories'].split('|')\n",
    "            for cl in result['categories']:\n",
    "              #catss = {make_filename(result['name']):str(result['name']}\n",
    "              #catss_copy = catss.copy()\n",
    "              #cats[ck].append(catss_copy)\n",
    "              #cats[ck][make_filename(result['name'])] = str(result['name'])\n",
    "              \n",
    "              # Using the categories as a toggle hack (by Joi or not)\n",
    "              if cl == \"_mine\":\n",
    "                result['type'] = cl\n",
    "\n",
    "              if cl not in cats.keys():\n",
    "                cats[cl] = {}\n",
    "\n",
    "              if fileName not in cats[cl].keys():\n",
    "                cats[cl][fileName] = str(result['name'])\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Sources\n",
    "    if result[\"source\"] == \"\":\n",
    "        result[\"source\"] = None \n",
    "    if result[\"source_url\"] == \"\":\n",
    "        result[\"source_url\"] = None \n",
    "\n",
    "    # end of RESULT Items FOR loop }\n",
    "    # --------------------------------------------------------------------------------------\n",
    "  \n",
    "\n",
    "    # --------------------------------------------------------------------------------------\n",
    "    # Temporary, parse out the name:filename dictionary into a list of key:value dicts\n",
    "    # I am doing this to preserve legacy in the Jekyl templates. May remove laters.\n",
    "    for k,v in sorted(result['photos_dict'].items()):\n",
    "        phots = {'filename':v,'name':k}\n",
    "        result['photos'].append(phots)\n",
    "    # Delete the concatened phot_names and photo_filenames string we got from the SQL query\n",
    "    del result['photos_names'],result['photos_filenames']\n",
    "    # --------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    # Convert the data struct to JSON and dump it to individual files\n",
    "    json_dump = json.dumps(result, ensure_ascii=False, sort_keys=True, indent=2)\n",
    "    jsonFilePath = path_output_json_files + fileName + \".json\"\n",
    "    f = open(jsonFilePath, 'w')\n",
    "    f.write(json_dump)\n",
    "    f.close()\n",
    "\n",
    "    # Prepare and Dump the Markodwn Recipe Stub Files\n",
    "    # MARKDOWN --------------------------------\n",
    "    # Create a string of Markdown\n",
    "    # So this will require some \"design.\" What do we want to include from the export?\n",
    "    # How should it be styled? What do we jam into the metadata/frontmatter\n",
    "    # What do we include as #tags in the body?\n",
    "\n",
    "    output  = \"---\\n\"\n",
    "    output += \"title: \\\"\" + result['name'] + \"\\\"\\n\"\n",
    "    output += \"filename: \\\"\" + fileName + \"\\\"\\n\"\n",
    "    output += \"created: \" + str(result['created']) + \"\\n\"\n",
    "    output += \"---\\n\"\n",
    "    if (result['notes']):\n",
    "      output += str(result['notes']) + \"\\n\"\n",
    "    \n",
    "    # Create/Open a text file for each recipe and write the above Markdown string into it\n",
    "    mdFilePath = path_output_mkdn_files + fileName + \".md\"\n",
    "    f = open(mdFilePath, 'w')\n",
    "    f.write(output)\n",
    "    f.close()\n",
    "    \n",
    "# CLOSE DB\n",
    "conn.close()\n",
    "\n",
    "\n",
    "# Convert the data struct to JSON and dump it to individual files\n",
    "json_cats_dump = json.dumps(cats, ensure_ascii=False, sort_keys=True, indent=2)\n",
    "jsonDataPath = path_output_json_data + \"recipe_categories.json\"\n",
    "f = open(jsonDataPath, 'w')\n",
    "f.write(json_cats_dump)\n",
    "f.close()\n",
    "\n",
    "\n",
    "# End of Main RESULTS FOR Loop }\n",
    "    \n",
    "#pp = pprint.PrettyPrinter(indent=4)\n",
    "#pp.pprint(cats)\n",
    "#print(cats)\n",
    "\n",
    "# CLEANUP --------------------------------------------\n",
    "# Delete the temp working direcotry\n",
    "shutil.rmtree(path_temp_working, ignore_errors=True) # \"ignore errors\" nukes it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGES ----------------------------------------------\n",
    "# Move the images out of the unzipped My Recipes dir to somehwere Jekyll can pick them up.\n",
    "\n",
    "if os.path.exists(path_output_phot_files):\n",
    "    shutil.rmtree(path_output_phot_files, ignore_errors=True)\n",
    "    #print(\"Nuked Recipe / Images Directory\")\n",
    "\n",
    "moveReturn = shutil.copytree(path_photos, path_output_phot_files)\n",
    "#print(\"Successfully copied to destination path:\", moveReturn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DONE!\n",
    "üëçüèº"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}